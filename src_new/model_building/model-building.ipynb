{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../../material/data/news_dataset.json') as json_file:  \n",
    "    data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data['articles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "#allowed_categories = ['POLITICS', 'TRAVEL', 'SPORTS', 'RELIGION', 'SCIENCE', 'TECH', 'ARTS']\n",
    "allowed_categories = {'POLITICS': 0, 'ENTERTAINMENT': 0, 'HEALTHY LIVING': 0, 'TRAVEL': 0, 'BUSINESS': 0, 'SPORTS':0, 'SCIENCE': 0}\n",
    "\n",
    "#max_per_cat = 1000\n",
    "\n",
    "filtered_data = []\n",
    "filter_date = datetime.strptime('2017-08-01', \"%Y-%m-%d\")\n",
    "\n",
    "for dct in data:\n",
    "    if dct['category'] in allowed_categories:\n",
    "        datetime_object = datetime.strptime(dct['date'], '%Y-%m-%d')\n",
    "        if dct['category'] in ['POLITICS', 'ENTERTAINMENT']:\n",
    "            if datetime_object >= filter_date:\n",
    "                filtered_data.append(dct)\n",
    "                allowed_categories[dct['category']]+=1\n",
    "        else:\n",
    "            filtered_data.append(dct)\n",
    "            allowed_categories[dct['category']]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'POLITICS': 7049,\n",
       " 'ENTERTAINMENT': 3058,\n",
       " 'HEALTHY LIVING': 6694,\n",
       " 'TRAVEL': 9887,\n",
       " 'BUSINESS': 5937,\n",
       " 'SPORTS': 4884,\n",
       " 'SCIENCE': 2178}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "allowed_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "politics = []\n",
    "entertainment = []\n",
    "health = []\n",
    "travel = []\n",
    "business = []\n",
    "sports = []\n",
    "science = []\n",
    "\n",
    "for dct in filtered_data:\n",
    "    if dct['category'] == \"POLITICS\":\n",
    "        politics.append(tuple((str(dct['headline']) + '. ' + str(dct['short_description']), \"POLITICS\")))\n",
    "    if dct['category'] == \"ENTERTAINMENT\":\n",
    "        entertainment.append(tuple((str(dct['headline']) + '. ' + str(dct['short_description']), \"ENTERTAINMENT\")))\n",
    "    if dct['category'] == \"HEALTHY LIVING\":\n",
    "        health.append(tuple((str(dct['headline']) + '. ' + str(dct['short_description']), \"HEALTH\")))\n",
    "    if dct['category'] == \"TRAVEL\":\n",
    "        travel.append(tuple((str(dct['headline']) + '. ' + str(dct['short_description']), \"TRAVEL\")))\n",
    "    if dct['category'] == \"BUSINESS\":\n",
    "        business.append(tuple((str(dct['headline']) + '. ' + str(dct['short_description']), \"BUSINESS\")))\n",
    "    if dct['category'] == \"SPORTS\":\n",
    "        sports.append(tuple((str(dct['headline']) + '. ' + str(dct['short_description']), \"SPORTS\")))\n",
    "    if dct['category'] == \"SCIENCE\":\n",
    "        science.append(tuple((str(dct['headline']) + '. ' + str(dct['short_description']), \"SCIENCE\")))\n",
    "        \n",
    "max_samples = 2000\n",
    "politics = random.sample(politics, max_samples)\n",
    "entertainment = random.sample(entertainment, max_samples)\n",
    "health = random.sample(health, max_samples)\n",
    "travel = random.sample(travel, max_samples)\n",
    "business = random.sample(business, max_samples)\n",
    "sports = random.sample(sports, max_samples)\n",
    "science = random.sample(science, max_samples)\n",
    "\n",
    "articles = politics + entertainment + health + travel + business + sports + science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_samples_per_cat(samples, cats):\n",
    "    counts = {}\n",
    "    for name in cats:\n",
    "        counts[name] = 0\n",
    "    \n",
    "    for corpus, label in samples:\n",
    "        counts[label]+=1\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POLITICS': 2000, 'ENTERTAINMENT': 2000, 'HEALTH': 2000, 'TRAVEL': 2000, 'BUSINESS': 2000, 'SPORTS': 2000, 'SCIENCE': 2000}\n"
     ]
    }
   ],
   "source": [
    "print(count_samples_per_cat(articles, ['POLITICS', 'ENTERTAINMENT', 'HEALTH', 'TRAVEL', 'BUSINESS', 'SPORTS', 'SCIENCE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return \" \".join(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_articles = []\n",
    "for corpus, label in articles:\n",
    "    p_corpus = preprocess_document(corpus)\n",
    "    p_articles.append(tuple((p_corpus, label)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from numpy import array\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "X = array([corpus for corpus, label in articles])\n",
    "y = [label for corpus, label in articles]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=12)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'POLITICS': 1599, 'ENTERTAINMENT': 1620, 'HEALTH': 1612, 'TRAVEL': 1596, 'BUSINESS': 1605, 'SPORTS': 1579, 'SCIENCE': 1589}\n",
      "{'POLITICS': 401, 'ENTERTAINMENT': 380, 'HEALTH': 388, 'TRAVEL': 404, 'BUSINESS': 395, 'SPORTS': 421, 'SCIENCE': 411}\n"
     ]
    }
   ],
   "source": [
    "print(count_samples_per_cat(zip(X_train, y_train), ['POLITICS', 'ENTERTAINMENT', 'HEALTH', 'TRAVEL', 'BUSINESS', 'SPORTS', 'SCIENCE']))\n",
    "print(count_samples_per_cat(zip(X_test, y_test), ['POLITICS', 'ENTERTAINMENT', 'HEALTH', 'TRAVEL', 'BUSINESS', 'SPORTS', 'SCIENCE']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building - Multinomial Naive Bayes\n",
    "\n",
    "This is the final model chosen to be integrated with the application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "mnb_classifier = MultinomialNB()\n",
    "\n",
    "X_train = tfidf.fit_transform(X_train).toarray()\n",
    "\n",
    "mnb_classifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[288   9  47  23  11   5  12]\n",
      " [  2 300  12  31   7  15  13]\n",
      " [ 23  13 305   8  13   4  22]\n",
      " [ 25  10  12 340   6   6   2]\n",
      " [ 13   8  54   5 306   9  16]\n",
      " [  3  27  14   7   5 361   4]\n",
      " [ 11  10  18   4   8   4 349]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BUSINESS       0.79      0.73      0.76       395\n",
      "ENTERTAINMENT       0.80      0.79      0.79       380\n",
      "       HEALTH       0.66      0.79      0.72       388\n",
      "     POLITICS       0.81      0.85      0.83       401\n",
      "      SCIENCE       0.86      0.74      0.80       411\n",
      "       SPORTS       0.89      0.86      0.88       421\n",
      "       TRAVEL       0.83      0.86      0.85       404\n",
      "\n",
      "    micro avg       0.80      0.80      0.80      2800\n",
      "    macro avg       0.81      0.80      0.80      2800\n",
      " weighted avg       0.81      0.80      0.80      2800\n",
      "\n",
      "0.8032142857142858\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "X_test = tfidf.transform(X_test).toarray()\n",
    "\n",
    "y_pred = mnb_classifier.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(mnb_classifier, open(\"classifier.model\", 'wb'))\n",
    "pickle.dump(tfidf, open(\"tfidf_kaggledataset.model\",'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unseen Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib3 as urllib\n",
    "urllib.disable_warnings()\n",
    "import xmltodict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes used:  POLITICS ENTERTAINMENT HEALTH TRAVEL BUSINESS SPORTS SCIENCE\n"
     ]
    }
   ],
   "source": [
    "print(\"Classes used: \",'POLITICS', 'ENTERTAINMENT', 'HEALTH', 'TRAVEL', 'BUSINESS', 'SPORTS', 'SCIENCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyt_retrieve(nyt_rss_url, label): # retrieve articles from New York Times\n",
    "    articles = []\n",
    "    \n",
    "    # code dependent on the nytimes structure of RSS feed\n",
    "    http = urllib.PoolManager()\n",
    "    r = http.request('GET', nyt_rss_url)\n",
    "\n",
    "    data = xmltodict.parse(r.data)\n",
    "    data = data[\"rss\"]\n",
    "    data = data[\"channel\"]\n",
    "    data = data[\"item\"]\n",
    "\n",
    "    for key in data:\n",
    "        article = key\n",
    "        title, descr, extra_descr = \"\", \"\", \"\"\n",
    "        if \"title\" in article and article[\"title\"] is not None:\n",
    "            title = article[\"title\"] + \". \"\n",
    "        if \"media:description\" in article and article[\"media:description\"] is not None:\n",
    "            descr = article[\"media:description\"]\n",
    "        if \"description\" in article and article[\"description\"] is not None:\n",
    "            extra_descr = article[\"description\"]\n",
    "\n",
    "        corpus = str(title) + str(descr) + str(extra_descr)\n",
    "        articles.append(tuple((corpus, label)))\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt = []\n",
    "nyt += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Politics.xml', 'POLITICS')\n",
    "nyt += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Movies.xml', 'ENTERTAINMENT')\n",
    "nyt += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Health.xml', 'HEALTH')\n",
    "nyt += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Travel.xml', 'TRAVEL')\n",
    "nyt += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Business.xml', 'BUSINESS')\n",
    "nyt += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Sports.xml', 'SPORTS')\n",
    "nyt += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Science.xml', 'SCIENCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_nyt = []\n",
    "for corpus, label in nyt:\n",
    "    p_nyt.append(tuple((preprocess_document(corpus),label)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unseen, y_unseen = [], []\n",
    "for corpus, label in p_nyt:\n",
    "    X_unseen.append(corpus)\n",
    "    y_unseen.append(label)\n",
    "\n",
    "X_unseen = tfidf.transform(X_unseen).toarray()\n",
    "\n",
    "y_pred = mnb_classifier.predict(X_unseen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[33  1  2  4  3  2  3]\n",
      " [ 3 23  2  1  0  5  3]\n",
      " [ 5  0  6  2  0  0  0]\n",
      " [ 0  1  0 16  0  1  2]\n",
      " [ 5  0  5  6  8  1  2]\n",
      " [ 0  0  0  0  0 19  1]\n",
      " [ 0  0  1  1  0  0 18]]\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "     BUSINESS       0.72      0.69      0.70        48\n",
      "ENTERTAINMENT       0.92      0.62      0.74        37\n",
      "       HEALTH       0.38      0.46      0.41        13\n",
      "     POLITICS       0.53      0.80      0.64        20\n",
      "      SCIENCE       0.73      0.30      0.42        27\n",
      "       SPORTS       0.68      0.95      0.79        20\n",
      "       TRAVEL       0.62      0.90      0.73        20\n",
      "\n",
      "    micro avg       0.66      0.66      0.66       185\n",
      "    macro avg       0.65      0.67      0.64       185\n",
      " weighted avg       0.70      0.66      0.66       185\n",
      "\n",
      "0.6648648648648648\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(y_unseen,y_pred))  \n",
    "print(classification_report(y_unseen,y_pred))  \n",
    "print(accuracy_score(y_unseen, y_pred))  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
