{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Profile Based Retrieval System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\manue\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from gensim import corpora, models, similarities\n",
    "from operator import itemgetter\n",
    "import nltk\n",
    "import urllib3 as urllib\n",
    "urllib.disable_warnings()\n",
    "import xmltodict\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_document(doc):\n",
    "    stopset = set(stopwords.words('english'))\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = wordpunct_tokenize(doc)\n",
    "    clean = [token.lower() for token in tokens if token.lower() not in stopset and len(token) > 2]\n",
    "    final = [stemmer.stem(word) for word in clean]\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_category_samples(articles, category):\n",
    "    counts = {}\n",
    "    for name in category:\n",
    "        counts[name] = 0\n",
    "        \n",
    "    for corpus, label in articles:\n",
    "        counts[label] += 1\n",
    "        \n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples(samples, division): # consistent split\n",
    "    \n",
    "    samples_p = [(corpus, label) for (corpus, label) in samples if label == 'Politics']\n",
    "    samples_b = [(corpus, label) for (corpus, label) in samples if label == 'Business']\n",
    "    samples_t = [(corpus, label) for (corpus, label) in samples if label == 'Tech']\n",
    "    samples_s = [(corpus, label) for (corpus, label) in samples if label == 'Science']\n",
    "    samples_h = [(corpus, label) for (corpus, label) in samples if label == 'Health']\n",
    "    samples_sp = [(corpus, label) for (corpus, label) in samples if label == 'Sports']\n",
    "    samples_a = [(corpus, label) for (corpus, label) in samples if label == 'Arts']\n",
    "    \n",
    "    train_p = samples_p[:int(division*len(samples_p))]\n",
    "    test_p = samples_p[int(division*len(samples_p)):]\n",
    "    \n",
    "    train_b = samples_b[:int(division*len(samples_b))]\n",
    "    test_b = samples_b[int(division*len(samples_b)):]\n",
    "    \n",
    "    train_t = samples_t[:int(division*len(samples_t))]\n",
    "    test_t = samples_t[int(division*len(samples_t)):]\n",
    "    \n",
    "    train_s = samples_s[:int(division*len(samples_s))]\n",
    "    test_s = samples_s[int(division*len(samples_s)):]\n",
    "    \n",
    "    train_h = samples_h[:int(division*len(samples_h))]\n",
    "    test_h = samples_h[int(division*len(samples_h)):]\n",
    "    \n",
    "    train_sp = samples_sp[:int(division*len(samples_sp))]\n",
    "    test_sp = samples_sp[int(division*len(samples_sp)):]\n",
    "\n",
    "    train_a = samples_a[:int(division*len(samples_a))]\n",
    "    test_a = samples_a[int(division*len(samples_a)):]\n",
    "    \n",
    "    trainset = train_p + train_b + train_t + train_s + train_h + train_sp + train_a\n",
    "    testset = test_p + test_b + test_t + test_s + test_h + test_sp + test_a\n",
    "    \n",
    "    return trainset, testset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information retrieval\n",
    "\n",
    "This time retrieve from saved dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [] # container for all the retrieved articles in the form corpus-category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nyt_retrieve(nyt_rss_url, label): # retrieve articles from New York Times\n",
    "    articles = []\n",
    "    \n",
    "    # code dependent on the nytimes structure of RSS feed\n",
    "    http = urllib.PoolManager()\n",
    "    r = http.request('GET', nyt_rss_url)\n",
    "\n",
    "    data = xmltodict.parse(r.data)\n",
    "    data = data[\"rss\"]\n",
    "    data = data[\"channel\"]\n",
    "    data = data[\"item\"]\n",
    "\n",
    "    for key in data:\n",
    "        article = key\n",
    "        title, descr, extra_descr = \"\", \"\", \"\"\n",
    "        if \"title\" in article and article[\"title\"] is not None:\n",
    "            title = article[\"title\"] + \". \"\n",
    "        if \"media:description\" in article and article[\"media:description\"] is not None:\n",
    "            descr = article[\"media:description\"]\n",
    "        if \"description\" in article and article[\"description\"] is not None:\n",
    "            extra_descr = article[\"description\"]\n",
    "\n",
    "        corpus = str(title) + str(descr) + str(extra_descr)\n",
    "        articles.append(tuple((corpus, label)))\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bbc_retrieve(rss_url, label): # retrieve from BBC\n",
    "    articles = []\n",
    "    \n",
    "    http = urllib.PoolManager()\n",
    "    r = http.request('GET', rss_url)\n",
    "    \n",
    "    data = xmltodict.parse(r.data)\n",
    "    data = data[\"rss\"]\n",
    "    data = data[\"channel\"]\n",
    "    data = data[\"item\"]\n",
    "    \n",
    "    for key in data:\n",
    "        article = key\n",
    "        title,descr = '', ''\n",
    "        if 'title' in article and article['title'] is not None:\n",
    "            title = article['title']+'. '\n",
    "        if 'description' in article and article['description'] is not None:\n",
    "            descr = article['description']\n",
    "\n",
    "        corpus = str(title) + str(descr)\n",
    "        articles.append(tuple((corpus, label)))\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def theguardian_retrieve(rss_url, label): # retrieve from TheGuardian\n",
    "    articles = []\n",
    "    \n",
    "    http = urllib.PoolManager()\n",
    "    r = http.request('GET', rss_url)\n",
    "    \n",
    "    data = xmltodict.parse(r.data)\n",
    "    data = data[\"rss\"]\n",
    "    data = data[\"channel\"]\n",
    "    data = data[\"item\"]\n",
    "    \n",
    "    for key in data:\n",
    "        article = key\n",
    "        title,descr = '', ''\n",
    "        if 'title' in article and article['title'] is not None:\n",
    "            title = article['title']\n",
    "            \n",
    "        corpus = str(title)\n",
    "        articles.append(tuple((corpus, label)))\n",
    "        \n",
    "    return articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles += theguardian_retrieve('https://www.theguardian.com/politics/rss', 'Politics')\n",
    "articles += theguardian_retrieve('https://www.theguardian.com/uk/business/rss', 'Business')\n",
    "articles += theguardian_retrieve('https://www.theguardian.com/uk/technology/rss', 'Tech')\n",
    "articles += theguardian_retrieve('https://www.theguardian.com/science/rss', 'Science')\n",
    "articles += theguardian_retrieve('https://www.theguardian.com/lifeandstyle/health-and-wellbeing/rss', 'Health')\n",
    "articles += theguardian_retrieve('https://www.theguardian.com/uk/sport/rss', 'Sports')\n",
    "articles += theguardian_retrieve('https://www.theguardian.com/uk/culture/rss', 'Arts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles += bbc_retrieve('http://feeds.bbci.co.uk/news/politics/rss.xml', 'Politics')\n",
    "articles += bbc_retrieve('http://feeds.bbci.co.uk/news/business/rss.xml', 'Business')\n",
    "articles += bbc_retrieve('http://feeds.bbci.co.uk/news/technology/rss.xml', 'Tech')\n",
    "articles += bbc_retrieve('http://feeds.bbci.co.uk/news/science_and_environment/rss.xml', 'Science')\n",
    "articles += bbc_retrieve('http://feeds.bbci.co.uk/news/health/rss.xml', 'Health')\n",
    "articles += bbc_retrieve('http://feeds.bbci.co.uk/sport/rss.xml', 'Sports')\n",
    "articles += bbc_retrieve('http://feeds.bbci.co.uk/news/entertainment_and_arts/rss.xml', 'Arts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Politics.xml', 'Politics')\n",
    "articles += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Business.xml', 'Business')\n",
    "articles += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Technology.xml', 'Tech')\n",
    "articles += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Science.xml', 'Science')\n",
    "articles += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Health.xml', 'Health')\n",
    "articles += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Sports.xml', 'Sports')\n",
    "articles += nyt_retrieve('http://rss.nytimes.com/services/xml/rss/nyt/Arts.xml', 'Arts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "879"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(articles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Corpus of the articles are cleaned applying stop words removal, PorterStemmer, tokenization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(['farag', 'brexit', 'parti', 'use', 'poll', 'oust', 'remain', 'parliament'], 'Politics'), (['chang', 'elect', 'candid', 'step', 'offens', 'tweet'], 'Politics')]\n",
      "879\n"
     ]
    }
   ],
   "source": [
    "p_articles = []\n",
    "for corpus, title in articles:\n",
    "    p_corpus = preprocess_document(corpus)\n",
    "    p_articles.append(tuple((p_corpus, title)))\n",
    "\n",
    "print(p_articles[1:3])\n",
    "print(len(p_articles))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature extraction\n",
    "\n",
    "### Create dictionary for Tf-Idf scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corpus_list = [corpus for (corpus, label) in p_articles]\n",
    "\n",
    "#dictionary = corpora.Dictionary(corpus_list)\n",
    "\n",
    "#print(len(dictionary))\n",
    "\n",
    "# adjust no_below, no_above\n",
    "#dictionary.filter_extremes(no_below = 5, no_above = 0.9, keep_n=2000)\n",
    "\n",
    "#print(len(dictionary))\n",
    "\n",
    "#dictionary.save('articles-dict.dict')\n",
    "#dictionary.save_as_text('dict_preview.txt', sort_by_word=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['sturgeon', 'outlin', 'new', 'scottish', 'independ', 'referendum', 'plan'], 'Politics')\n",
      "sturgeon outlin new scottish independ referendum plan Politics\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer  \n",
    "from sklearn.feature_extraction.text import TfidfTransformer  \n",
    "\n",
    "corpus_list = [\" \".join(corpus) for (corpus, label) in p_articles]\n",
    "y = [label for (corpus, label) in p_articles]\n",
    "\n",
    "# integrity check\n",
    "print(p_articles[0])\n",
    "print(corpus_list[0], y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(879, 15169)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vectorizer = CountVectorizer(max_features=2000, min_df=0, max_df=1, stop_words=stopwords.words('english'))  \n",
    "#X = vectorizer.fit_transform(corpus_list).toarray()  \n",
    "\n",
    "#tfidfconverter = TfidfTransformer()  \n",
    "#X = tfidfconverter.fit_transform(X).toarray()  \n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(sublinear_tf=True, min_df=1, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\n",
    "\n",
    "X = tfidf.fit_transform(corpus_list).toarray()\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Politics': 105, 'Business': 128, 'Tech': 131, 'Health': 66, 'Science': 89, 'Sports': 75, 'Arts': 109}\n",
      "{'Politics': 33, 'Business': 40, 'Tech': 25, 'Health': 14, 'Science': 16, 'Sports': 16, 'Arts': 32}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)  \n",
    "\n",
    "print(count_category_samples(zip(X_train, y_train), ['Politics','Business','Tech','Health','Science','Sports','Arts']))\n",
    "print(count_category_samples(zip(X_test, y_test), ['Politics','Business','Tech','Health','Science','Sports','Arts']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(X_train[1], y_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='auto',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#classifier = RandomForestClassifier(n_estimators=1000, random_state=0)  \n",
    "classifier = LogisticRegression(solver='lbfgs', multi_class = 'auto')\n",
    "classifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[18  5  0  0  0  0  9]\n",
      " [ 0 23  0  3  2  0 12]\n",
      " [ 0 10  0  0  0  0  4]\n",
      " [ 1  8  0 20  0  0  4]\n",
      " [ 1  6  1  2  2  0  4]\n",
      " [ 0  8  0  0  0  3  5]\n",
      " [ 1 10  0  1  0  0 13]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Arts       0.86      0.56      0.68        32\n",
      "    Business       0.33      0.57      0.42        40\n",
      "      Health       0.00      0.00      0.00        14\n",
      "    Politics       0.77      0.61      0.68        33\n",
      "     Science       0.50      0.12      0.20        16\n",
      "      Sports       1.00      0.19      0.32        16\n",
      "        Tech       0.25      0.52      0.34        25\n",
      "\n",
      "   micro avg       0.45      0.45      0.45       176\n",
      "   macro avg       0.53      0.37      0.38       176\n",
      "weighted avg       0.55      0.45      0.44       176\n",
      "\n",
      "0.44886363636363635\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))  \n",
    "print(classification_report(y_test,y_pred))  \n",
    "print(accuracy_score(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_category_samples(X_train, 'Politics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Docs2Bows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectors = [dictionary.doc2bow(corpus) for (corpus,label) in p_articles]\n",
    "corpora.MmCorpus.serialize('articles-vsm.mm', vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = models.TfidfModel(vectors)\n",
    "\n",
    "import random\n",
    "\n",
    "random.shuffle(p_articles)\n",
    "\n",
    "featureset = []\n",
    "for corpus, label in p_articles:\n",
    "    vcorpus = dictionary.doc2bow(corpus)\n",
    "    ctfidf = tfidf[vcorpus]\n",
    "    feature_row = {}\n",
    "    for i in range(0,len(dictionary)):\n",
    "        feature_row[i] = 0\n",
    "    \n",
    "    for key,value in ctfidf:\n",
    "        feature_row[key] = value\n",
    "    featureset.append((feature_row, label))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(featureset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this data we create train set (80%) and test set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "trainset, testset = split_samples(featureset, 0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier\n",
    "\n",
    "### TF-IDF Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_classifier = nltk.NaiveBayesClassifier.train(trainset)\n",
    "print(\"TF-IDF: \"+str(nltk.classify.accuracy(bayes_classifier, testset)))\n",
    "bayes_classifier.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_classifier_m = nltk.NaiveBayesClassifier.train(trainset_m)\n",
    "print(\"M: \"+str(nltk.classify.accuracy(bayes_classifier_m, testset_m)))\n",
    "bayes_classifier_m.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Multinomial Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bayes_classifier_mn = nltk.NaiveBayesClassifier.train(trainset_mn)\n",
    "print(\"MNorm: \"+str(nltk.classify.accuracy(bayes_classifier_mn, testset_mn)))\n",
    "bayes_classifier_mn.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Entropy Classification\n",
    "\n",
    "### Multi-Variate Bernoulli Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent_classifier_mvb = nltk.classify.MaxentClassifier.train(trainset_mvb, 'IIS', trace=0, max_iter=5)\n",
    "print(\"MVB: \"+str(nltk.classify.accuracy(maxent_classifier_mvb, testset_mvb)))\n",
    "maxent_classifier_mvb.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent_classifier_m = nltk.classify.MaxentClassifier.train(trainset_m, 'IIS', trace=0, max_iter=5)\n",
    "print(\"M: \"+str(nltk.classify.accuracy(maxent_classifier_m, testset_m)))\n",
    "maxent_classifier_m.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Multinomial Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxent_classifier_mn = nltk.classify.MaxentClassifier.train(trainset_mn, 'IIS', trace=0, max_iter=5)\n",
    "print(\"MNorm: \"+str(nltk.classify.accuracy(maxent_classifier_mn, testset_mn)))\n",
    "maxent_classifier_mn.show_most_informative_features(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier\n",
    "\n",
    "### Multi-Variate Bernoulli Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest_classifier_mvb = nltk.classify.DecisionTreeClassifier.train(trainset_mvb, entropy_cutoff=0, support_cutoff=0)\n",
    "print(\"MVB: \"+str(nltk.classify.accuracy(randforest_classifier_mvb, testset_mvb)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randforest_classifier_m = nltk.classify.DecisionTreeClassifier.train(trainset_m, entropy_cutoff=0, support_cutoff=0)\n",
    "print(\"M: \"+str(nltk.classify.accuracy(randforest_classifier_m, testset_m)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalized Multinomial Feature Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "randforest_classifier_mn = nltk.classify.DecisionTreeClassifier.train(trainset_mn, entropy_cutoff=0, support_cutoff=0)\n",
    "print(\"MNorm: \"+str(nltk.classify.accuracy(randforest_classifier_mn, testset_mn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Deployment\n",
    "Since it is the best performing model among the ones I tried, I choose to use the Naive Bayes Classifier trained with Multi-Variate Bernoulli Feature Set to be used into the profile-based retrieval system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open('nbayes-mvb.pickle', 'wb')\n",
    "pickle.dump(bayes_classifier_mvb, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######### how to load a saved model? \n",
    "# import pickle\n",
    "# f = open('my_classifier.pickle', 'rb')\n",
    "# classifier = pickle.load(f)\n",
    "# f.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
